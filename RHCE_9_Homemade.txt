 +-+-+ +-+-+-+ +-+-+-+-+ +-+-+ +-+-+-+-+ +-+-+-+ +-+-+-+-+ +-+-+-+-+-+-+-+-+-+ +-+-+-+ +-+-+-+-+ +-+-+-+-+-+-+-+-+
 |I|n| |t|h|e| |n|a|m|e| |o|f| |G|o|d|,| |t|h|e| |M|o|s|t| |G|r|a|c|i|o|u|s|,| |t|h|e| |M|o|s|t| |M|e|r|c|i|f|u|l|
 +-+-+ +-+-+-+ +-+-+-+-+ +-+-+ +-+-+-+-+ +-+-+-+ +-+-+-+-+ +-+-+-+-+-+-+-+-+-+ +-+-+-+ +-+-+-+-+ +-+-+-+-+-+-+-+-+
----------------------------------------------------------------------------------------------------------------------------------------------
         __________																													                                                                                                                                     |
        /\____;;___\																														  |		                                                                                                                                      |
       /         /																														                                                                                                                                       |
       `. ())oo() . 																													                                                                                                                                       |
        \(%()*^^()^\																														                                                                                                                                      |
       %| |-%-------																														                                                                                                                                       |
      % \ | %  ))   |																														                                                                                                                                        |
      %  \|%________|																													                                                                                                                                        |
       %%%%              ~ Good Luck ~                                                                                                        |
     _       _       _       _                                                                                                                |
   _( )__  _( )__  _( )__  _( )__                                                                                                             |
 _|     _||     _||     _||     _|                                                                                                            |
(_ R _ ((_ H _ ((_ C _ ((_ E _ (_                                                                                                             |    
  |_( )__||_( )__||_( )__||_( )__|                                                                                                            |
----------------------------------------------------------------------------------------------------------------------------------------------
PART 0 : Nodes Initial Setup & Configuration                                                                                                  |
----------------------------------------------------------------------------------------------------------------------------------------------

Ansible is an open-source automation tool used for configuration management, application deployment, and task automation. Its architecture is designed to be simple and efficient, focusing on ease of use and scalability. Here’s a high-level overview of Ansible's architecture:

I. Key Components:

1) Control Node (MASTER) (Ansible Node):

This is the machine where Ansible is installed and run. It is responsible for executing playbooks, managing inventories, and sending commands to managed nodes. The control node contains the Ansible command-line tools and Python libraries required for its operation.


2) Managed Nodes (SLAVES) (Target Nodes):
These are the target systems or servers that Ansible configures or manages. Managed nodes do not need to have Ansible installed; they only need to have SSH access (or another connection method like WinRM for Windows systems) so that Ansible can communicate with them.
Inventory:


3) The inventory is a file (usually in INI or YAML format) that lists all the managed nodes. It can be static or dynamic, depending on whether it’s manually configured or generated by scripts or cloud providers. The inventory defines groups of nodes and their properties, which helps in organizing and targeting specific groups for configuration.
Playbooks:


4) Playbooks are YAML files that define a set of tasks to be executed on managed nodes. They describe the desired state of the system, including tasks like installing packages, copying files, and managing services. Playbooks are a central part of Ansible's configuration management and automation process.
Modules:


5) Modules are the units of work that Ansible uses to perform tasks on managed nodes. They are executed on the managed nodes by the control node. Ansible has a wide range of built-in modules for tasks like file manipulation, user management, and service handling. You can also create custom modules if needed.
Tasks:


6) Tasks are individual actions or operations defined within playbooks. Each task calls a module and specifies its parameters to perform a particular operation on the managed nodes.
Roles:


7) Roles are a way to organize and reuse playbooks. They provide a structured approach to grouping related tasks, variables, and handlers into reusable components. Roles help in managing complex configurations by modularizing the playbooks into smaller, manageable pieces.
Variables:


8) Variables in Ansible allow for the dynamic configuration of playbooks and roles. They can be defined in playbooks, inventory files, or separate variable files. Variables enable the customization of tasks and configurations based on different environments or conditions.
Handlers:


9) Handlers are special tasks in playbooks that are triggered by other tasks. They are usually used to restart services or perform actions that should only occur if a change is made, helping to ensure that tasks are executed only when necessary.


II. Communication:

1) SSH (for Linux/Unix systems):
Ansible uses SSH to connect to and manage Linux/Unix-based managed nodes. It does not require agents or additional software on the managed nodes, as it leverages SSH for communication.
WinRM (for Windows systems):

2) For Windows systems:
For windows managed nodes, Ansible uses the Windows Remote Management (WinRM) protocol to execute commands and manage configurations.

Ansible’s architecture emphasizes simplicity, making it easy to use and deploy. Its agentless nature reduces overhead and simplifies management, while its modular approach provides flexibility and scalability for automation tasks.

Step I: Control Node 

1) Hosts Definition
# vim /etc/hosts
192.168.241.100   control.tekup.com    control
192.168.241.10    ansible1.tekup.com   ansible1
192.168.241.20    ansible2.tekup.com   ansible2
192.168.241.30    ansible3.tekup.com   ansible3
192.168.241.40    ansible4.tekup.com   ansible4


2) IP Assignment
# nmtui:
 ╤ IPv4 CONFIGURATION <Automatic> 
   Addresses 192.168.241.20

3) Install ansible in controle node:
# dnf install ansible-core
# dnf install python3-pip
# python3 -m pip install ansible-navigator

4) Create user to work with in all machines:
# useradd ansible
# passwd ansible

5) SSH without password from the controle node:
# su - ansible
# ssh-keygen
# ssh-copy-id ansible1
# ssh-copy-id ansible2
# ssh-copy-id ansible3
# ssh-copy-id ansible4

ssh ansible@ansibleX

6) Add privilege root to user ansible
# echo "ansible ALL=(ALL)  NOPASSWD: ALL" > /etc/sudoers.d/ansible


Step II: ansibleX Nodes:
1) IP Assignment
# nmtui: 
 ╤ IPv4 CONFIGURATION <Automatic> 
   Addresses 192.168.1.X0

2) SSH Configuration:
# vim /etc/ssh/sshd_config
		Port 22
		PermitRootLogin yes
		StrictModes no
        StrictHostKeyChecking no # In case setup frequently changes
        UserKnownHostsFile=/dev/null
Cntrl + S & Cntrl + X
# systemctl restart --now sshd

3) In all ansible nodes:
# useradd ansible
# passwd ansible
# systemctl restart --now sshd
# vim /etc/sudoers.d/ansible
ansible   ALL=(ALL)   NOPASSWD: ALL

[i] IF YOU'RE ENCOUNTRING THIS ERROR [!]
[automation@control ~]$ ssh-copy-id managed2
/usr/bin/ssh-copy-id: INFO: Source of key(s) to be installed: "/home/automation/.ssh/id_rsa.pub"
/usr/bin/ssh-copy-id: INFO: attempting to log in with the new key(s), to filter out any that are already installed

/usr/bin/ssh-copy-id: ERROR: @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
ERROR: @    WARNING: REMOTE HOST IDENTIFICATION HAS CHANGED!     @
ERROR: @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@
ERROR: IT IS POSSIBLE THAT SOMEONE IS DOING SOMETHING NASTY!
ERROR: Someone could be eavesdropping on you right now (man-in-the-middle attack)!
ERROR: It is also possible that a host key has just been changed.
ERROR: The fingerprint for the ED25519 key sent by the remote host is
ERROR: SHA256:Y7Rr8Cu0FgIz6IhF9FZ2NZPUbhKzZ3B5seBP96rMXVg.
ERROR: Please contact your system administrator.
ERROR: Add correct host key in /home/automation/.ssh/known_hosts to get rid of this message.
ERROR: Offending ED25519 key in /home/automation/.ssh/known_hosts:18
ERROR: Host key for managed2 has changed and you have requested strict checking.
ERROR: Host key verification failed.

--> Solution 1:
# ssh-keygen -R managed2
# ssh-copy-id managed2

--> Solution 2:
# vim ~/.ssh/known_hosts
< Delete the line containing managed2 >
# ssh-copy-id managed2



Step III: Control Node 
1) Inventory Setup : Basic
# su - ansible
# vim inventory
    ansible1
    ansible2
    ansible3
    ansible4

# ansible-inventory -i inventory --list

Output:
{
    "_meta": {
        "hostvars": {}
    },
    "all": {
        "children": [
            "ungrouped"
        ]
    },
    "ungrouped": {
        "hosts": [
            "ansible1",
            "ansible2",
            "ansible3",
            "ansible4"
        ]
    }
}



# ansible all -i inventory -m ping          # The 'all' keyword will execute the specified command on all hosts found in the inventory file
Output:
ansible2 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "ping": "pong"
}
ansible4 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "ping": "pong"
}
ansible1 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "ping": "pong"
}
ansible3 | SUCCESS => {
    "ansible_facts": {
        "discovered_interpreter_python": "/usr/bin/python3"
    },
    "changed": false,
    "ping": "pong"
}

1') Inventory Setup : Advanced 
# vim inventory 
    ansible4 # Ungrouped
    [dev]
    ansible1
    [test]
    ansible2
    [production]
    ansible3
    [servers:children] # Define a group named servers that has 2 sub-groups dev & test
    dev
    test



2) ansible.cfg Setup:
# vim ansible.cfg
    [defaults]
    remote_user = ansible
    host_key_checking = false
    ask_pass= false
    inventory = inventory

    [privilege_escalation]
    become=true
    become_user=root
    become_method=sudo
    become_ask_pass=false


# ansible-inventory --graph
Output:
@all:
  |--@ungrouped:
  |  |--ansible4
  |--@production:
  |  |--ansible3
  |--@servers:
  |  |--@dev:
  |  |  |--ansible1
  |  |--@test:
  |  |  |--ansible2


3) Additional Module
# ansible-galaxy collection install ansible.posix -p /home/ansible/ansible/collection


3.1) Ping every node that belongs to the "servers" group [ansible1 & ansible2]
# ansible servers -m ping                   # We no longer need to specify inventory, its part of the ansible.cfg file


[!] Since this part is added in the ansible.cfg file:
    [privilege_escalation]
    become=true
    become_user=root
    become_method=sudo
    become_ask_pass=false

[!] We can execute the command witout '-b' flag 
# ansible servers -m reboot

3.3) Install Additionnal Modules: (selinux etc..)
# ansible-galaxy collection install ansible.posix

[!] To find out HOW TO USE any Module [!]
# ansible-doc -l | grep $MODULE_NAME


----------------------------------------------------------------------------------------------------------------------------------------------
PART 1 : Ad-hoc Commands VS Playbooks                                                                                                         |
----------------------------------------------------------------------------------------------------------------------------------------------

0) Host Variables & Var Files & Vars Files

a. Host Variables:

hosts: all --> Targets all hosts defined in the inventory

hosts: host1 --> Targets a specific host named host1 

hosts: webservers --> Targets a group of hosts named webservers defined in the inventory

hosts: webservers:dbservers --> Targets hosts in either of the specified groups

hosts: webservers:&dbservers --> Targets hosts that are part of both specified groups

hosts: webservers:!dbservers --> Targets hosts in a group but excludes those in another

hosts: 'web*' --> Targets all hosts matching the wildcard pattern

hosts: web[1:5] --> Targets hosts named within a specific range

b. Var Files:

Ansible will look through all the specified vars_files to find the variables, and if a variable is defined in multiple files, the last file that defines the variable will overwrite the other.This means you can’t have two variables with the same name in different files if you want both values to be used.

** Include a single variables files
vars_files:
        - /path/to/vars.yml

OR 

** Include multiple variables files
vars_files:
        - /path/to/vars1.yml
        - /path/to/vars2.yml
eg: 

The contents of each variables file is a simple YAML dictionary. For example:

somevar: value1
password: magic

---
- name: Example Playbook
  hosts: all

  vars_files:
    - vars1.yml
    - vars2.yml

  tasks:
    - name: Print a variable from vars1.yml
      debug:
        msg: "{{ somevar }}"   # value1 will be printed

    - name: Print a variable from vars2.yml
      debug:
        msg: "{{ password | password_hash('256') }}"   # magic will be hashed then printed

c. Vars Files:

Include a single variable 
vars:
    variable_name: variable_value

OR

Include multiple variables

vars:
    variable1: value1
    variable2: value2
    variable3: value3

eg: 

---
- name: Example Playbook using vars
  hosts: all

  vars:
    variable1: value1
    variable2: value2
    variable3: value3

  tasks:
    - name: Print variable1
      debug:
        msg: "variable1 is {{ variable1 }}"

    - name: Print variable2
      debug:
        msg: "variable2 is {{ variable2 }}"

    - name: Print variable3
      debug:
        msg: "variable3 is {{ variable3 }}"


1) Conditions & Magic Variables & Facts

a. Conditions

    + when: "'production' in group_names" --> Check for a specific value in group_names

    + when: ansible_os_family == 'Debian' --> Use Ansible facts

    + when: my_variable == 'some_value' --> Check the value of a variable

    + when: not my_condition --> Negate a condition

    + when: ansible_distribution == 'Ubuntu' and ansible_distribution_version == '20.04' --> Combine conditions with logical operators

    + when: ansible_facts['file_exists'] --> Check if a file exists

    + when: my_host_var is defined and my_host_var == 'some_value' --> Evaluate host variables

    + when: result.changed --> Use changed status of a task (after registering it in a variable named "register")


b. Magic Variables & Facts

[!] Note: Magic Variables are to memorize [!]

inventory_hostname -->  Specifies the inventory hostname for the current host
inventory_file --> Provides the name of the inventory in use
hostvars --> Contains all the hosts in the inventory and their variables
groups --> Contains all the groups in the inventory
group_names --> Gives the groups to which a host belongs


c. Facts

Facts in Ansible are pieces of information about the remote system that Ansible gathers automatically when it connects to a host. 

To view these facts, you can run the command:
# ansible all -m setup | less


2) Converting YAML Playbooks to Ad-hoc Commands

Task 1: Install httpd on ALL nodes
+ ansible.builtin.dnf
# ansible-doc -l | grep dnf
# ansible-doc ansible.builtin.dnf
    TYPE '/' and SEARCH for "EXAMPLE" and hit Enter
    COPY this part from the examples provided in the docs: 
        //
        - name: Install the latest version of Apache
        ansible.builtin.dnf:
          name: httpd
          state: present
        //

[!] PLAYBOOK: 

- name: Install the latest version of Apache
  ansible.builtin.dnf
    name: httpd
    state: latest

[!] COMMAND:  -a "name=httpd state=present"

# ansible all -m dnf -a "name=httpd state=present"


Task 2: Enable & Start httpd Service
+ ansible.builtin.service
# ansible-doc -l | grep service
# ansible-doc service
    TYPE '/' and SEARCH for "EXAMPLE" and hit Enter
    COPY  this part from the examples provided in the docs:
        //
        -name: ENABLE service httpd, and not touch the state
        ansible.builtin.service:
          name: httpd
          enabled: yes
        &
        - name: START service httpd, if not started
        ansible.builtin.service:
          name: httpd
          state: started
        //

[!] PLAYBOOK: 
- name: Start service httpd, if not started
  ansible.builtin.service:
    name: httpd
    state: started
    enabled: yes

[!] COMMAND: -a "name=httpd state=started enabled=yes"

# ansible all -m ansible.builtin.service -a "name=httpd state=started enabled=yes"


Task 3: Remotely Creating a File Without Content
+ ansible.builtin.file
# ansible-doc -l | grep file
# ansible-doc file
TYPE '/' and SEARCH for "EXAMPLE" and hit Enter  
COPY this part from the examples provided in the docs:  
    //  
    - name: Ensure a file exists without content  
      ansible.builtin.file:  
        path: /root/Desktop/test.conf  
        state: touch  
        mode: '0666'  
    //  

[!] PLAYBOOK:

- name: Ensure a file exists without content
  ansible.builtin.file:
    path: /root/Desktop/test.conf
    state: touch
    mode: '0666'

[!] COMMAND: -a "path=/root/Desktop/test.conf state=touch mode=666"

# ansible ansible1 -m ansible.builtin.file -a "path=/root/Desktop/test.conf state=touch mode=666"

Task 4: Remotely Creating a File with Content
+ ansible.builtin.copy
# ansible-doc -l | grep copy
# ansible-doc copy

TYPE '/' and SEARCH for "EXAMPLE" and hit Enter  
COPY this part from the examples provided in the docs:  
    //  
    - name: Copy a file with content to a remote destination  
      ansible.builtin.copy:  
        content: 'Hello World!'  
        dest: /root/Desktop/test.conf  
    //  

[!] PLAYBOOK:

- name: Copy a file with content to a remote destination
  ansible.builtin.copy:
    content: 'Hello World!'
    dest: /root/Desktop/test.conf

[!] COMMAND: -a "content='Hello World!' dest=/root/Desktop/test.conf"

# ansible ansible1 -m ansible.builtin.copy -a "content='Hello World!' dest=/root/Desktop/test.conf"


Task 5: Copying a Local File to One of the Slave Nodes
+ ansible.builtin.copy
# ansible-doc -l | grep copy
# ansible-doc copy

TYPE '/' and SEARCH for "EXAMPLE" and hit Enter  
COPY this part from the examples provided in the docs:  
    //  
    - name: Copy a file from the local system to the remote system  
      ansible.builtin.copy:  
        src: source.conf  
        dest: /root/Desktop/source.conf  
        owner: ansible  
        mode: '0644'  
    //  

[!] PLAYBOOK:

- name: Copy a file from the local system to the remote system
  ansible.builtin.copy:
    src: source.conf
    dest: /root/Desktop/source.conf
    owner: ansible
    mode: '0644'

[!] COMMAND: -a "src=source.conf dest=/root/Desktop/source.conf owner=ansible mode='0644'"

# ansible ansible1 -m ansible.builtin.copy -a "src=source.conf dest=/root/Desktop/source.conf owner=ansible mode='0644'"

Task 6: Executing curl Remotely
+ ansible.builtin.uri
# ansible-doc -l | grep uri
# ansible-doc uri

TYPE '/' and SEARCH for "EXAMPLE" and hit Enter  
COPY this part from the examples provided in the docs:  
    //  
    - name: Get content from a URL  
      ansible.builtin.uri:  
        url: http://localhost  
        return_content: true  
    //  

[!] PLAYBOOK:
- name: Get content from a URL
  uri:
    url: http://localhost
    return_content: true

[!] COMMAND: -a "url=http://localhost return_content=true"

# ansible ansible1 -m uri -a "url=http://localhost return_content=true"


Task 7: Change a Line in a File
+ ansible.builtin.lineinfile
# ansible-doc -l | grep lineinfile
# ansible-doc lineinfile

TYPE '/' and SEARCH for "EXAMPLE" and hit Enter  
COPY this part from the examples provided in the docs:  
    //  
    - name: Ensure SELinux is enforcing in the configuration file  
      ansible.builtin.lineinfile:  
        path: /etc/selinux/config  
        regexp: '^SELINUX='  
        line: SELINUX=enforcing 
    //  

[!] PLAYBOOK:
- name: Ensure SELinux is enforcing in the configuration file
  lineinfile:
    path: /etc/selinux/config
    regexp: '^SELINUX='
    line: SELINUX=enforcing

[!] COMMAND: -a "path=/etc/selinux/config regexp='^SELINUX=' line='SELINUX=enforcing'"

# ansible ansible1 -m ansible.builtin.lineinfile -a "path=/etc/selinux/config regexp='^SELINUX=' line='SELINUX=enforcing'"
Task: Manipulation of ansible with Ansible playbook


3) Advanced Playbooks

Lab 0: Your assignment is to configure a web server using Ansible for a group of hosts designated as "servers."
• You must install the Apache web server, ensure it is configured to start automatically at boot, and allow traffic through the firewall on a specified port (Port 82).
• Additionally, you will add content to the server’s default page and apply the necessary SELinux context for the web directory. 
• If the SELinux context is modified, the Apache service should be restarted via a handler. 
• Finally, verify that the server is up and running by sending a request to the homepage.

# vim playbookLab0.yaml

---
- name: Playbook to configure a web server
  hosts: servers
  tasks:
    - name: Install the latest version of Apache
      dnf:
        name: httpd
        state: latest

    - name: Enable & Start httpd Service
      service:
        name: httpd
        state: started
        enabled: yes

    - name: Permit Traffic in default zone for http service
      firewalld:
        service: http
        permanent: true
        immediate: true # --now
        state: enabled

    - name: Default page
      copy:
        content: 'Web Server Automated!'
        dest: /var/www/html/index.html

    - name: Check that a page returns the content
      uri:
        url: http://localhost
        return_content: true
      register: result
        # ignore_errors: yes

    - name: Print return information from the previous task
      debug:
        var: result
                                

# ansible-playbook playbook0.yaml


Lab1: Complete the following tasks in a single playbook named playbookLab1.yaml 
• Use the dnf module to install the httpd package on all servers in the "prod" group.
• Run a command to check the disk usage (df -h) on all servers in the "ops" group.
• Restart the httpd service on all servers in the "prod" group.
• Update the packages on all servers in the "servers" group.
• Create a user with the name "testuser" and set their password to "pass123", their shell to “/bin/sh” and a comment “this is a test user”.
• Change the permissions of the file `/var/log/secure` to 600.
• Create a directory with the name "data" in the `/opt` directory.
• Copy all files under /etc from the control machine to /opt/data directory on the managed host that belongs to "servers" group


# vim playbookLab1.yaml
---
- name: Playbook to test out magic vars & facts
  hosts: all
  tasks:
  - name: Install the latest version of Apache on hosts in the prod group
    ansible.builtin.dnf:
      name: httpd
      state: latest
    when: ansible_hostname in groups.prod

  - name: Disk Usage Checking on hosts in the ops group
    command: df -h
    when: ansible_hostname in groups.ops
    register: result

  - name: Restart service httpd, in all cases on hosts in the prod group
    service:
      name: httpd
      state: restarted
    when: ansible_hostname in groups.prod

  - name: Update all packages on hosts in the servers group
    yum:
      name: '*'
      state: latest
    when: ansible_hostname in groups.servers

  - name: Add the user testuser, set a password, their shell to /bin/sh, and a mock comment
    user:
      name: testuser
      comment: This is a test user
      shell: /bin/sh
      password: "{{ 'pass123' | password_hash('512') }}"

  - name: Change file ownership, group and permissions
    file:
      path: /var/log/secure
      mode: '0600'

  - name: Create a directory named data in the /opt directory if it does not exist
    file:
      path: /opt/data
      state: directory

  - name: Copy all files under /etc/ from machines in the servers group to /opt/data on the control node
    copy:
      src: /etc/     # Note the '/' 
      dest: /opt/data/{{ ansible_hostname }}
      remote_src: yes      # So that the "copy" module searches in remote hosts
    when: ansible_hostname in groups.servers

# ansible-playbook playbookLab1.yaml --syntax-check                    


[!] To Set a Password for a newly created user [!]

-> 2 Possible Methods:

1. SIMPLE BUT NOT SECURE:
---
- name: Playbook for Lab1
  hosts: all
  vars: 
    userpass : password
  tasks:

  - name: Creating a test user
    ansible.builtin.user:
      name: testuser
      shell: /bin/sh
      comment: "this is a test user"
      password: "{{ userpass | password_hash('sha512') }}"

2. COMPLEX BUT SECURE:
a) Create a secrets.yaml file in the same directory
b) Inside the secrets.yaml: 
    Define a variable (in our case "userpass")
    Assign to this variable the desired password
    EXAMPLE:
    userpass: pass123
c) Inside the Playbook 
---
- name: Playbook for Lab1
  hosts: all
  vars_files:
  - secrets.yaml
  tasks:
  - name: Creating a test user
    ansible.builtin.user:
      name: testuser
      shell: /bin/sh
      comment: "this is a test user"
      password: "{{ userpass | password_hash('sha512') }}"


Lab1': Create a file in /home/ansible called report.yml. Using this playbook, create a file called report.txt in ansible1 and ansible2. 
The file content of report.txt is: (Should be placed on ansible1 & ansible2)
HOST= inventory hostname
MEMORY=total memory in mb
BIOS=bios version
SDA_DISK_SIZE=disk size
SDB_DISK_SIZE=disk size

Then edit the lines in the file to provide the real information of the hosts.

# vim /home/ansible/report.yml

---
- name: Playbook to modify report.txt file on ansible1 & ansible2
  hosts: ansible1,ansible2
  tasks:
    - name: Loading Hostname
      lineinfile:
        path: /home/ansible/reports.txt
        regexp: '^HOST='
        line: HOST= "{{ inventory_hostname }}"

    - name: Loading Memory
      lineinfile:
        path: /home/ansible/reports.txt
        regexp: '^MEMORY='
        line: MEMORY= "{{ ansible_memtotal_mb }}"

    - name: Loading BIOS Version
      lineinfile:
        path: /home/ansible/reports.txt
        regexp: '^BIOS='
        line: BIOS= "{{ ansible_bios_version }} "

    - name: Loading SDA Size
      lineinfile:
        path: /home/ansible/reports.txt
        regexp: '^SDA_DISK_SIZE='
        line: SDA_DISK_SIZE= "{{ ansible_devices.sda.size }}"

    - name: Loading SDB Size
      lineinfile:
        path: /home/ansible/reports.txt
        regexp: '^SDB_DISK_SIZE='
        line: SDB_DISK_SIZE= "{{ ansible_devices.sdb.size }}"







Lab2: Create an Ansible playbook named playbookLab2.yml to automate the creation of users on machines in different groups. 
• The playbook must add user1 to all machines in the "prod" group, setting their shell to /bin/sh, adding the comment "This is a test user", and assigning a password hashed with SHA-256, stored in a variable file secrets.yaml. 
• Additionally, add user2 to all machines in the "ops" group, setting their shell to /sbin/nologin to prevent interactive login, with their password also hashed using SHA-256 and stored in the same variable file.

# vim secrets.yaml
user1pass: pass1*
user2pass: pass2*

# vim playbookLab2.yaml

---
- name: Playbook to automate the creation of users on machines in different groups
  hosts: all
  vars_files:
    - /home/ansible/secrets.yaml
  tasks:
    - name: Add the user user1 on machines from group prod, set a password, their shell to /bin/sh, and a mock comment
      user:
        name: user1
        shell: /bin/sh
        comment: "This is a test user"
        password: "{{ user1pass | password_hash('256') }}"
      when: ansible_hostname in groups.prod

    - name: Add user user2 on machines from group ops, set the password, their shell should prevent interactive login
      user:
        name: user2
        shell: /sbin/nologin
        password: "{{ user2pass | password_hash('256') }}"
      when: ansible_hostname in groups.ops

# ansible-playbook playbookLab2.yaml --syntax-check


Lab3: Write an Ansible playbook named playbookLab3.yml that targets the hosts on the 'ops' group and achieves the following tasks
• Installs a list of packages (httpd, firewalld, vsftpd). 
• Once the packages are installed, ensure that the corresponding services are started and running. Use a loop to iterate over the list of packages for both installation and service management tasks.

# vim playbookLab3.yaml
---
- name: Playbook to automated installation of three packages on hosts belonging to the ops group
  hosts: all
  vars:
    packages:
    - httpd
    - firewalld
    - vsftpd
  tasks:
    - name: Package Installation
      dnf:
        name: "{{ item }}"
        state: present
      loop: "{{ packages }}"
      when: ansible_hostname in groups.ops

    - name: Starting & Enabling installed Services
      service:
        name: "{{ item }}"
        state: started
        enabled: yes
      loop: "{{ packages }}"
      when: "'ops' in group_names"   # Just another way of writing ansible_hostname in groups.ops

# ansible-playbook playbookLab3.yaml --syntax-check


Lab4: Write an Ansible playbook named playbookLab4.yml that performs the following tasks on all target hosts. 
• Load variables from an external file located at /home/ansible/vars.txt. 
• The playbook should first create a group with the name defined in the group_name variable. 
• Then, it should create a user with the following properties: username, home directory, and shell, as specified in the variable file. 
• Ensure that the user is added to the newly created group.
• vars.txt content:
        username: "mark"
        home_directory: "/home/mark"
        user_shell: "/sbin/nologin"
        group_name: "markGroup"


# vim playbookLab4.yaml

---
- name: Playbook
  hosts: all
  vars_files:
    - /home/ansible/vars.txt
  tasks:
    - name: Group Creation
      group:
        name: "{{ group_name }}"
        state: present
      when: ansible_hostname in groups.ops
      ignore_errors: yes

    - name: User Mark Creation
      user:
        name: "{{ username }}"
        shell: "{{ user_shell }}"
        group: "{{ group_name }}"
        home: "{{ home_directory }}"
      when: ansible_hostname in groups.ops

    - name: Verifying User Infos
      command:  grep "{{ username }}" /etc/passwd
      register: result
      when: ansible_hostname in groups.ops

    - name: Debugging Result
      debug:
        var: result
      when: ansible_hostname in groups.ops
      

Lab5: Create an Ansible playbook named playbookLab5.yml that performs the following tasks on hosts that belongs to 'ops' group.
• First, create a new primary partition of size 850MiB on /dev/sda. 
• Then, format the partition with the ext4 filesystem. 
• Create a directory /mnt/mp1 to be used as the mount point, and mount the newly created partition to this directory, ensuring that the mount is persistent across reboots. 
• Verify the changes by executing the lsblk command on /dev/sda, and display the result using the debug module. Ensure all tasks are conditionally executed only on hosts that belongs to 'opt' group
• Do the same tasks on hosts that belongs to group 'prod' and to formatted as an xfs filesystem with 500 MiB size. Make sure it's mounted persistently across reboots.

# vim playbookLab5.yaml

---
- name: Playbook to Create an ext4 partition and an xfs partition on managed hosts that belongs to "prod" and "ops" groups
  hosts: all
  tasks:
  - name: Create a new primary partition on sda block
    parted:
      device: /dev/sda
      number: 1
      state: present
      part_end: 850MiB
    when: inventory_hostname in groups.ops and "'sda' in ansible devices"

  - name: Formatting the partition to ext4 format
    filesystem:
      fstype: ext4
      dev: /dev/sda1
    when: inventory_hostname in groups.ops

  - name: Create a new mountpoint mp1
    file:
      path: /mnt/mp1
      state: directory
    when: inventory_hostname in groups.ops

  - name: Mounting the ext4 partition on mp1 mountpoint
    mount:
      src: /dev/sda1
      path: /mnt/mp1
      fstype: ext4
      state: mounted
    when: inventory_hostname in groups.ops

  - name: Verifying
    command: lsblk /dev/sda1
    register: var1
    when: inventory_hostname in groups.ops

  - name: Debugging
    debug:
      var: var1
    when: inventory_hostname in groups.ops

  - name: Create a primary partition on sdb block
    parted:
      device: /dev/sdb
      number: 1
      state: present
      part_end: 500MiB
    when: inventory_hostname in groups.prod and "'sdb' in ansible devices"

  - name: Formatting to xfs
    filesystem:
      fstype: xfs
      dev: /dev/sdb1
    when: inventory_hostname in groups.prod

  - name: Create a new mountpoint mp1
    file:
      path: /mnt/mp1
      state: directory
    when: inventory_hostname in groups.prod

  - name: Mounting the xfs partition on mp1 mountpoint
    mount:
      src: /dev/sdb1
      path: /mnt/mp1
      fstype: xfs
      state: mounted
    when: inventory_hostname in groups.prod

  - name: Verifying
    command: lsblk /dev/sdb1
    register: var2
    when: inventory_hostname in groups.prod

  - name: Debugging
    debug:
      var: var2
    when: inventory_hostname in groups.prod

# ansible-playbook playbookLab5.yaml --syntax-check 


Lab6: Create an Ansible playbook named playbookLab6.yml that performs the following tasks on hosts that belong to the 'servers' group.

First, create a new partition of size 500MiB on /dev/sdb.
Then, create a Volume Group (VG) named vg1 using the new partition (/dev/sdb1) as the physical volume.
Next, create a Logical Volume (LV) named lv1 of size 250MiB within the VG vg1.
Format the Logical Volume with the ext4 filesystem, ensuring that it resizes as necessary.
Create a mount point directory at /mnt/mountpoint, and mount the logical volume on this directory, ensuring that the mount is persistent across reboots by adding it to /etc/fstab.
Finally, verify the changes by executing the lsblk command on /dev/vg1/lv1, and display the result using the debug module.
Ensure all tasks are conditionally executed only on hosts in the 'servers' group.


# vim playbookLab6.yaml

---
- name: Basic LVM Playbook
  hosts: all
  tasks:
    - name: New Partition Creation on /dev/sdb with a 500MiB size
      parted:
        device: /dev/sdb
        number: 1
        state: present
        part_end: 500MiB
      when: ansible_hostname in groups.servers

    - name: Create a volume group VG named vg1 using the physical volume /dev/sdb1
      lvg:
        vg: vg1
        pvs: /dev/sdb1
      when: ansible_hostname in groups.servers

    - name: Create a logical volume LV named lv1 using the volume group vg1 with a 250MiB size
      lvol:
        vg: vg1
        lv: lv1
        size: 250MiB
      when: ansible_hostname in groups.servers

    - name: Formatting the LV with ext4 filesystem and resize it if necessary
      filesystem:
        dev: /dev/vg1/lv1
        fstype: ext4
        resize2fs: true
      when: ansible_hostname in groups.servers

    - name: Creating a mount point directory
      file:
        path: /mnt/mountpoint
        state: directory
      when: ansible_hostname in groups.servers

    - name: Mounting lv1 on fstab
      mount:
        path: /mnt/mountpoint
        src: /dev/vg1/lv1
        fstype: ext4
        opts: defaults
        state: mounted
      when: ansible_hostname in groups.servers

    - name: Verification
      command: lsblk /dev/vg1/lv1
      register: var1
      when: ansible_hostname in groups.servers

    - name: Debugging
      debug:
        var: var1
      when: ansible_hostname in groups.servers

# ansible-playbook playbookLab6.yaml --syntax-check 


----------------------------------------------------------------------------------------------------------------------------------------------
PART 2 : Error Handling and Recovery: Block Fail Rescue Always                                                                                |
----------------------------------------------------------------------------------------------------------------------------------------------

- block:
    # Tasks to execute inside the block. These are the main tasks you want to run.
    - fail:
        msg: "Condition not met"  # Example failure message
      when: some_condition_is_not_met  # Example condition to check
    
    - fail:
        msg: "Another failure condition"
      when: another_condition_is_not_met

  rescue:
    # Tasks to run if the block fails. These are your backup or corrective tasks.
    - name: Run a recovery task
      some_module:
        param1: value1
        param2: value2

    - name: Run another recovery task
      another_module:
        param: value
    
  always:
    # Tasks to always run, regardless of success or failure.
    - name: Ensure a resource or action always happens
      some_module:
        param1: value1
        param2: value2

    - name: Another always-run task
      another_module:
        param: value


Lab7: Complete the following tasks: 

Task-1:
Create an Ansible playbook named playbookLab7-init.yaml that performs the following tasks on hosts belonging to the "lvm_group" group.
For the host ansible1:
Create a new partition of size 2GiB on /dev/sdb.

For the host ansible3:
Create a new partition of size 1GiB on /dev/sdb.

After creating the partitions on both hosts, create a Volume Group (VG) named ansible_vg using the partition /dev/sdb1 as the physical volume.


Task-2:
Create an Ansible playbook named playbookLab7.yaml that performs the following tasks on hosts belonging to the lvm_group group.
Verify the following conditions:

Fail the task if the host does not have the device /dev/sdb.
Fail the task if the Volume Group ansible_vg does not exist.
Fail the task if there is not enough free space (at least 1.5GiB) in the Volume Group.
If the above conditions are met, proceed to create a Logical Volume (LV) named ansible_lv with a size of 1500MiB. If this fails due to insufficient space, attempt to create the LV with a size of 800MiB instead.

Regardless of the outcome of the previous tasks, ensure that:

A directory is created at /lv/.
The Logical Volume is formatted with the ext4 filesystem.
The Logical Volume is mounted at /lv, ensuring that it uses the defaults options.
Ensure that all tasks are conditionally executed only on hosts in the lvm_group group.  

# vim playbookLab7-T1.yaml

--- 
- name: Playbook for partitionning ansible1 and ansible3 to inint for the playbookLab7.yaml
  hosts: lvm_group
  tasks:

# ansible1 has vg “ansible_vg” uses /dev/sdb1 with size 2GB
# ansible3 has vg “ansible_vg” uses /dev/sdb1 with size 1GB

    - name: creating partition on ansible1
      parted:
        device: /dev/sdb
        number: 1
        state: present
        part_end: 2GiB
      when: ansible_hostname == "ansible1"

    - name: creating partition on ansible3
      parted:
        device: /dev/sdb
        number: 1
        state: present
        part_end: 1GiB
      when: ansible_hostname == "ansible3"
    
    - name: creating a vg for both hosts
      lvg:
        pvs: /dev/sdb1
        vg: ansible_vg

# ansible-playbook playbookLab7-T1.yaml --syntax-check 

# vim playbookLab7-T2.yaml

---
- name: Playbook to create LVM partitions on specific hosts
  hosts: lvm_group
  tasks:
      - fail: 
          msg: "Host does NOT have /dev/sdb"
        when: ansible_devices.sdb is not defined 

      - fail:
          msg: "ansible_vg does NOT exist"
        when: ansible_lvm.vgs.ansible_vg is not defined

    - block:

      - fail:
          msg: "Not enough space"
        when: (ansible_lvm.vgs.ansible_vg.free_g | float) < 1.5

      - lvol:
          vg: ansible_vg
          lv: ansible_lv
          size: 1500
      rescue:
      - lvol:
          vg: ansible_vg
          lv: ansible_lv
          size: 800

      always:
      - file:
          path: /lv/
          state: directory
          mode: '0777'

      - filesystem:
          fstype: ext4
          dev: /dev/ansible_vg/ansible_lv

      - mount:
          path: /lv/
          src: /dev/ansible_vg/ansible_lv
          fstype: ext4
          opts: defaults
          state: present


# ansible-playbook playbookLab7-T2.yaml --syntax-check 


----------------------------------------------------------------------------------------------------------------------------------------------
PART 3 : Force Handlers: Handlers & Nofity                                                                                                    |
----------------------------------------------------------------------------------------------------------------------------------------------


---
- hosts: <your_hosts>
  force_handlers: true  # Ensures handlers are run even if they are not changed
  tasks:
    - <task_name>:
        <task_parameters>
      notify:
        - <handler_name>

    - <another_task_name>:
        <another_task_parameters>

  handlers:
    - name: <handler_name>  # Name of the handler
      <module_name>:  # Ansible module to execute
        <module_parameters>  # Parameters for the module

Lab8: Your assignment is to create a user on a host designated as "ansible1" using Ansible.

You will create a user named "police" and set up a handler that triggers when the user is successfully created.
If the user creation fails, the playbook will halt and display an error message.
Upon successful user creation, a file named file1 should be created in the home directory of the user "police.

# vim playbookLab8.yaml

---
- name: Playbook to create a user and emulate force handlers
  hosts: all
  force_handlers: true
  tasks:
    - name: Creating a user Police
      user:
        name: police
      notify:
      - x1

    - fail:
        msg: user creation failed

  handlers:
    - name: x1
      file:
        path: /home/police/file1
        state: touch


# ansible-playbook playbook8.yaml


Lab9: Your assignment is to install and configure both a web server and an FTP server on all managed hosts.
• You will install the Apache web server (httpd) and the VSFTPD server.
• After installation, ensure that both services are started and enabled to run at boot.
• The web server should be configured to restart if any changes are made, as should the FTP server.

# vim playbookLab9.yaml

---
- name: Playbook to configure both a web server and an FTP server
  hosts: all
  vars:
    packages:
    - httpd # Must be at the same level as the var name
    - vsftpd # Must be at the same level as the var name
  tasks:
    - name: httpd and vsftpd installation
      dnf:
        name: "{{ item }}"
        state: latest
      loop: "{{ packages }}"

    - name: httpd and vsftpd enabling
      service:
        name: " {{ item }}"
        state: started
        enable: yes
      loop: "{{ packages }}"

    - name: modification on httpd.conf
      file:
        path: /etc/httpd/conf/httpd.conf
        state: modified
      notify: modification

    - name: modification on vsftpd.conf
      file:
        path: /etc/httpd/conf/httpd.conf
        state: modified
      notify: modification

  handlers:
    - name: modification
      service:
        name: "{{ item }}"
        state: restarted
      loop: "{{ packages }}"

# ansible-playbook playbookLab9.yaml --syntax-check


Lab10: Your assignment is to configure a web server using Ansible for a group of hosts designated as "servers."
• You must install the Apache web server, ensure it is configured to start automatically at boot, and allow traffic through the firewall on a specified port (Port 82).
• Additionally, you will add content to the server’s default page and apply the necessary SELinux context for the web directory. 
• If the SELinux context is modified, the Apache service should be restarted via a handler. 
• Finally, verify that the server is up and running by sending a request to the homepage.

# vim playbookLab10.yaml

--
- name: Playbook to configure a web server
  hosts: servers
  tasks:
    - name: Install the latest version of Apache
      dnf:
        name: httpd
        state: latest

    - name: Enable & Start httpd Service
      service:
        name: httpd
        state: started
        enabled: yes

    - name: Creating a directory for web server
      file:
        path: /opt/httpd
        state: directory

    - name: Changing default DocumentRoot path in the httpd config file
      lineinfile:
        path: /etc/httpd/conf/httpd.conf
        regexp: '^DocumentRoot "/var/www/html"'
        line: DocumentRoot "/opt/httpd"

    - name: Changing default directory path in the httpd config file
      lineinfile:
        path: /etc/httpd/conf/httpd.conf
        regexp: '^<Directory "/var/www">'
        line: <Directory "/opt/httpd">

    - name: Setting up a new port in the httpd config file
      lineinfile:
        path: /etc/httpd/conf/httpd.conf
        regexp: '^Listen'
        line: Listen 82

    - name: Changing default httpd port 80->82
      seport:
        ports: 82
        proto: tcp
        setype: http_port_t   # This var is available in ansible-docs seport
        state: present

    - name: Setting SELinux context for the new directory
      sefcontext:
        target: '/opt/httpd(/.*)?'        # This experession is available in ansible-docs sefcontext
        setype: httpd_sys_content_t   # This var is available in ansible-docs sefcontext
        state: present

    - name: Apply SELinux context
      command: restorecon -R -v /opt/httpd    # This can be found in ansible-docs sefcontext
      notify: port changed                    # If the restorecon command successfully modifies the SELinux context, it triggers the handler named "port changed"

    - name: Permit Traffic in default zone for http service
      firewalld:
        port: 82/tcp
        permanent: true
        immediate: true
        state: enabled

    - name: Restart httpd Service
      service:
        name: httpd
        state: restarted

    - name: Check that a page returns the content
      uri:
        url: http://localhost:82
        return_content: true
      register: result
    # ignore_errors: yes

    - name: Print return information from the previous task
      debug:
        var: result

  handlers:
    - name: port changed
      service:
        name: httpd
        state: restarted                                   

# ansible-playbook playbookLab10.yaml


----------------------------------------------------------------------------------------------------------------------------------------------
PART 4 :  Ansible Vault                                                                                                                       |
----------------------------------------------------------------------------------------------------------------------------------------------

# ansible-vault create password.yml
# ansible-vault view password.yml
# vim secrets.yaml 
user1pass: pass1*
user2pass: pass2*

# vim users.yml 
user1: "Bob"
user2: "Alice"

---
- name: Playbook to test out a basic ansible vault
  hosts: all
  vars_files:
    - users.yml
    - password.yml
  tasks:
    - name: Users creation
      user:
        name: "{{ users.yml }}"
        password: "{{ password.yml | password_hash(sha256) }}"
        state: present

# abnsible-playbook playbookLab11.yaml --ask-pass


Lab11: Create a variable file vault.yml and that file should contains the variable and its value.
pw_users is value user123
pw_admin is value admin123
i) vault.yml file should be encrpted using the password "pass123"
ii) Store the password in secret.txt file and which is used for encrypt the variable file.

# ansible-vault create vault.yml
Vault password: pass123

pw_users: "user123"
pw_admin: "admin123"

# vim secrets.txt 
pass123

# vim playbook.yaml
---
- name: playbook to debug 
  hosts: all
  vars_files:
    - vault.yml
  tasks:
    - debug:
       msg: "{{ pw_users }}"

    - debug:
       msg: "{{ pw_admin }}"

# ansible-playbook playbook.yml --vault-password-file=secrets.txt


Lab12: Your task is to create users using Ansible and manage their credentials securely with Vault.
Task 1: 
Create the file user_list.yml and its content is:
users:
- name: natasha
uid: 2222
password_expire_days: 3
job: manager
- name: adam
uid: 2223
password_expire_days: 5
job: developer

Task 1 Workload:
# vim user_list.yml 
users:
- name: natasha
  uid: 2222
  password_expire_days: 3
  job: manager

- name: adam
  uid: 2223
  password_expire_days: 5
  job: developer

Task 2:
Create a variable file named vault.yml containing the following variables and their values:
    - pw_developer with the value lamdev
    - pw_manager with the value lammgr
Encrypt the vault.yml file using the password "P@sswOrd".
Store this password in a file named secret.txt.

Task 2 Workload: 
# ansible-vault create vault.yml
New Vault password: P@sswOrd
Confirm New Vault password: P@sswOrd
pw_developer: "lamdev"
pw_manager: "lammgr"

# vim secret.txt
P@sswOrd

# ansible-vault view vault.yml --vault-password-file=secret.txt
pw_developer: "lamdev"
pw_manager: "lammgr"

Task 3:
Write a playbook named users.yml that will run on all managed nodes and use both vault.yml and user_list.yml as variable files.
a) For developers:
Create a group called opsdev on nodes that belongs to dev and test groups.
Create users from the users_list.yml file whose job is equal to developer and assign them to the opsdev group.
Use the pw_developer password (hashed using SHA512 format) for these users.
Run this part of the playbook only on hosts in the dev and test groups.

b) For managers:
Create a group called opsmgr on nodes that belongs to mgr group.
Create users from the users variable whose job is equal to manager and assign them to the opsmgr group.
Use the pw_manager password (hashed using SHA512 format) for these users.
Run this part of the playbook only on hosts in the test group.

c) Use the when condition to ensure that each play is executed only when the appropriate conditions are met (i.e., based on the user's job and the host's group).

Task 3 Workload: 

---
- name: Playbook to automate user creation using a secure vault
  hosts: all
  vars_files:
    - user_list.yml
    - vault.yml
  tasks:
    - name: Managers Group Creation
      group:
        name: opsmgr
        state: present
      when: inventory_hostname in groups.mgr

    - name: Developers Group Creation
      group:
        name: opsdev
        state: present
      when: inventory_hostname in groups.dev or inventory_hostname in groups.test

    - name: Dev Team Users Creation
      user:
        name: "{{ item.name }}"
        uid: "{{ item.uid }}"
        expires: "{{ item.password_expire_days }}"
        comment: "{{ item.job }}"
        group: opsdev
        password: "{{ pw_developer | password_hash('sha512') }}"
      loop: "{{ users }}"
      when: (inventory_hostname in groups.dev or inventory_hostname in groups.test) and item.job == 'developer'

    - name: Management Users Creation
      user:
        name: "{{ item.name }}"
        uid: "{{ item.uid }}"
        expires: "{{ item.password_expire_days }}"
        comment: "{{ item.job }}"
        group: opsmgr
        password: "{{ pw_manager | password_hash('sha512') }}"
      loop: "{{ users }}"
      when: inventory_hostname in groups.mgr and item.job == 'manager'

 
# ansible-playbook users.yml --vault-password-file=secret.txt --syntax-check

Task 4: Rekey the variable file vault.yml
- Old password: P@sswOrd
- New password: redhat

Task 4 Workload: 
# ansible-vault rekey vault.yml 
Vault password: P@sswOrd
New Vault password: redhat
Confirm New Vault password: redhat
Rekey successful

# vim secret.txt
redhat

# ansible-playbook users.yml --vault-password-file=secret.txt --syntax-check


----------------------------------------------------------------------------------------------------------------------------------------------
PART 4 :  Jinja Templates                                                                                                                      |
----------------------------------------------------------------------------------------------------------------------------------------------

Lab 13: 
Your task is to create a Jinja2 template and deploy it to the managed hosts using Ansible. Follow these steps:

Task 1:
On the control node, create a file named /root/myhosts.j2. This file should contain the following static entries at the top:
You should create the jinja template in this file, Then you should create a playbook in /home/ansible called hosts.yml on dev node so that /root/myhosts content will be like below. The order of the nodes doesn’t matter:
127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4
::1 localhost localhost.localdomain localhost6 localhost6.localdomain6

Task 1 Workload: 
# vim myhosts.j2 
127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4
::1 localhost localhost.localdomain localhost6 localhost6.localdomain6
{% for host in groups.all %}
{{hostvars[host].ansible_default_ipv4.address}} {{hostvars[host].ansible_fqdn}} {{hostvars[host].ansible_hostname}}
{% endfor %}


Task 2:
Next, create an Ansible playbook in /home/ansible on the dev node called hosts.yml. This playbook should apply the Jinja2 template and update the /etc/hosts file on the managed hosts with the following content:
10.0.2.1 node1.example.com node1
10.0.2.2 node2.example.com node2
10.0.2.3 node3.example.com node3
10.0.2.4 node4.example.com node4
10.0.2.5 node5.example.com node5

Task 2 Workload: 
# vim playbookLab12.yaml 
---
- name: Playbook to test out jinja
  hosts: all
  tasks: 
    - name: Apply the template on managed hosts
      template: 
        src: myhosts.j2
        dest: /etc/hosts
      when: inventory_hostname in groups.prod 

# ansible-playbook playbookLab12.yaml


Lab 14:
Your assignment is to create a custom /etc/hosts file on a hosts that belongs to dev group using Ansible.

You will first create a Jinja2 template named hosts.j2 that dynamically generates entries for each host in the Ansible inventory:
• Each entry should include the IP address, fully qualified domain name (FQDN), and the hostname of all managed host.
• The IP addresses must be gathered from the default IPv4 address of each host, and the FQDN and hostname should also be dynamically retrieved.

Once the template is created, write a playbook named playbookLab13.yml to apply the template to the /etc/hosts file on "ansible3."

# vim hosts.j2
{% for host in groups['all'] %}
{{ hostvars[host].ansible_default_ipv4.address }} {{ hostvars[host].ansible_fqdn }} {{ hostvars[host].ansible_hostname }}
{% endfor %}

# vim playbookLab14.yml 
---
- name: Playbook to create custom /etc/hosts file on dev group
  hosts: dev
  tasks:
    - name: Applying Created Template
      template: 
        src: hosts.j2
        dest: /etc/hosts


----------------------------------------------------------------------------------------------------------------------------------------------
PART 5:   Ansible Roles                                                                                                                       |
----------------------------------------------------------------------------------------------------------------------------------------------


Lab 15: Complete all tasks on control node.
I. Installing the Collection.

Task 1:
Create a directory "collections" under the /home/student/ansible.
Install the ansible.posix and the community.general collections under collections directory.

Task 1 Workload: 

# mkdir /home/student/ansible/collections
# ansible-galaxy collection install community.general
# ansible-galaxy collection install ansible.posix


[!] In the Exam [!]
# mkdir /home/student/ansible/collections
# ansible-galaxy collection install  http://content/Rhce/ansible-posix-1.4.0.tar.gz -p collections
# ansible-galaxy collection install  http://content/Rhce/redhat-rhel_system_roles-1.0.0.tar.gz -p collections

Task 2:
Install the system roles collection under collections directory.
Create a playbook name timesync.yml and use system roles.
- Use ntp server 172.25.254.254 and enable iburst.
- Run this playbook on all the managed nodes.

Task 2 Workload: 
# sudo dnf install rhel-system-roles -y 
# cd /usr/share/doc/rhel-system-roles/
# cd timesync/
# ls
# cat example-single-pool-playbook.yml 
---
- name: Example with single pool
  hosts: all
  vars:
    timesync_ntp_servers:
      - hostname: 2.pool.ntp.org
        pool: true
        iburst: true
  roles:
    - rhel-system-roles.timesync
# cp /usr/share/doc/rhel-system-roles/timesync/example-single-pool-playbook.yml /home/ansible/

# cp -avr /usr/share/ansible/roles/* /home/ansible/collections/
# ansible-galaxy install roles rhel-system-roles -p /home/ansible/collections/
# cd collections/
# vim timesync.yml
---
- name: Playbook to use timesync role and configure an NTP Server
  hosts: all
  vars:
    timesync_ntp_servers:
      - hostname: 172.25.254.254
        iburst: yes
  roles:
    - rhel-system-roles.timesync

# sudo ansible-playbook timesync.yml -i /home/ansible/inventory

[!] YOU DON'T HAVE TO MEMORIZE THIS PLAYBOOK [!]


Task 3:
Create a playbook name selinux.yml and use system roles
i) Set selinux mode as enforcing in all manage node

Task 3 Workload: 

# vim selinux.yml 

---
- name: Playbook to use selinux system role and set enforcing on all managed nodes
  hosts: all
  vars:
    - selinux_state: enforcing
  roles:
    - rhel-system-roles.selinux

[!] YOU DON'T HAVE TO MEMORIZE THIS PLAYBOOK [!]
[!] #cat rhel-system-roles.selinux/README.md | grep enforc [!]


II. Installing the roles.
Task 1:
Create a directory 'roles' under /home/student/ansible
Task 1 Workload:

# mkdir /home/student/ansible/roles

Task 2:
Create a playbook called requirements.yml under the roles directory and download the
given roles under the 'roles' directory using galaxy command under it.
iii) Role name should be balancer and download using this url WeslleyMiler.balancer
iv) Role name phpinfo and download using this url JelleBrouwer02.php

Task 2 Workload:
# vim /home/ansible/roles/requirements.yml
---
- name: balancer
  src: WeslleyMiler.balancer

- name: phpinfo
  src: JelleBrouwer02.php

# ansible-galaxy role install -r /home/ansible/roles/requirements.yml --force -p /home/ansible/roles


III. Using Custom Roles
Create a Playbook roles.yml for using the roles
1) The playbook contains the balancer hosts for use balancer role
    a) browsing, the balancers host group with url http://serverd.lab.example.com that produce the output
         "Welcome to servera.lab.example.com, (version 1.0)"
    b) Refreshing, the balancers host group with the same url the output should be change
         "Welcome to serverc.lab.example.com, (version 1.0)"

2) The playbook contains the webservers host group for using the role phpinfo
    a) browsing, the webserver host group name that provides the output
         "Welcome to serverc.lab.example.com, (version 1.0)"
       and the output comes with various php contents
    b) For example, the webserver hostgroup http://serverc.lab.example.com That provides the output
         "Welcome to serverc.lab.example.com, (version 1.0)"
       and the output comes with various php contents
    c) Similarly, the webserver hostgroup http://servera.lab.example.com that provides the output
         "My host is servera.lab.example.com on 172.25.250.10

# vim roles.yml
---
# Do not change the above roles order 
- name: Play to run balancer
  hosts: webservers
  become: true
  roles:
    - phpinfo #  to set up the web pages on the webservers
      

- name: Play to run
  hosts: webservers
  become: true
  roles:
    - balancer #  to load balance the requests across the webservers


IV. Offline Roles: 
Create offline role named apache under roles directory.

1) Install httpd package and the service should be start and enable the httpd service.
2) Host the web page using the template.j2
3) The template.j2 should contain i
       My host is  HOSTNAME  on IPADDRESS
       Where HOSTNAME is fully qualified domain name.
4) Create a playbook named apache_role.yml and run the role in prod group.

# ansible-galaxy init /home/ansible/roles/apache
# cd apache
# tree
.
├── apache
│ ├── defaults
│ │ └── main.yml
│ ├── files
│ ├── handlers
│ │ └── main.yml
│ ├── meta
│ │ └── main.yml
│ ├── README.md
│ ├── tasks
│ │ └── main.yml
│ ├── templates
│ ├── tests
│ │ ├── inventory
│ │ └── test.yml
│ └── vars
│     └── main.yml

# vim templates/template.j2

My host is {{ ansible_fqdn }} on {{ ansible_default_ipv4.address }}

# vim /tasks/main.yaml

- name: Install httpd package
- services:
      - httpd
      - firewalld

- dnf:
    name: {{ item }}
    state: present
  loop: {{ services }}

- service:
    name: {{ item }}
    state: started
    enabled: yes
  loop: {{ services }}

- firewalld:
    service: http
    state: enabled
    permanent: true
    immediate: true

- template: 
    src: /home/ansible/roles/apache/templates/template.j2
    dest: /var/www/html/index.html

# vim apache_role.yaml 
---
- name: Execution of Apache Role
  hosts: prod
  roles: 
    - apache

# ansible-playbook apache_role.yaml

# curl http://servera.lab.example.com

----------------------------------------------------------------------------------------------------------------------------------------------
PART 6:   Software Repositories: AppStream & BaseOS Repositories Configuration                                                                |
----------------------------------------------------------------------------------------------------------------------------------------------

Lab 16: Create a playbook named adhoc.yml to configure repositories on all nodes in an RHEL9 environment.

Repository 1: BaseOS
Name: baseos
Description: BaseOS Description
Base URL: http://content/rhel9.0/x86_64/dvd/BaseOS
GPG Check: Enabled
GPG Key URL: http://content.example.com/rhel9.0/x86_64/dvd/RPM-GPG-KEY-redhat-release
Repository Status: Enabled

Repository 2: AppStream
Name: appstream
Description: AppStream Description
Base URL: http://content/rhel9.0/x86_64/dvd/AppStream
GPG Check: Enabled
GPG Key URL: http://content.example.com/rhel9.0/x86_64/dvd/RPM-GPG-KEY-redhat-release
Repository Status: Enabled

# vim playbookLab16.yaml

---
- name: Playbook to configure repositories on all nodes
  hosts: all
  tasks:
    - name: Configure BaseOS Repo
      yum_repository:
        name: baseos
        description: "BaseOs Description"
        baseurl: http://content/rhel9.0/x86_64/dvd/BaseOS
        gpgcheck: yes
        gpgkey: http://content.example.com/rhel9.0/x86_64/dvd/RPM-GPG-KEY-redhat-release
        enabled: yes
          
    - name: Configure AppStream  Repo
      yum_repository:
        name: appstream
        description: "App Description"
        baseurl: http://content/rhel9.0/x86_64/dvd/AppStream
        gpgcheck: yes
        gpgkey: http://content.example.com/rhel9.0/x86_64/dvd/RPM-GPG-KEY-redhat-release
        enabled: yes

# ansible-playbook playbookLab16.yaml --syntax-check


Lab 17: Create a playbook playbookLab17.yaml that runs on servers in the dev host group and does the following:
A YUM repository file is created.
The name of the repository is mysql80-community.
The description of the repository is “MySQL 8.0 YUM Repo”.
Repository baseurl is http://repo.mysql.com/yum/mysql-8.0-community/el/8/x86_64/.
Repository GPG key is at http://repo.mysql.com/RPM-GPG-KEY-mysql.
Repository GPG check is enabled.
Repository is enabled.

# vim playbookLab17.yaml
---
- name: Playbook to configure mysql db software repository
  hosts: database
  tasks:
    - name: Create YUM repository for MySQL-8.0
      yum_repository:
        name: mysql80-community
        description: "MySQL 8.0 YUM Repo"
        baseurl: http://repo.mysql.com/yum/mysql-8.0-community/el/8/x86_64/
        gpgcheck: yes
        gpgkey: http://repo.mysql.com/RPM-GPG-KEY-mysql
        enabled: yes

# ansible-playbook playbookLab17.yaml --syntax-check



----------------------------------------------------------------------------------------------------------------------------------------------
PART 7:   Scheduled Tasks                                                                                                                     |
----------------------------------------------------------------------------------------------------------------------------------------------

Lab 18: Create a playbook regular_tasks.yml that runs on servers in the proxy host group and does the following:
A root crontab record is created that runs every hour.
The cron job appends the file /var/log/time.log with the output from the date command.


# vim playbookLab18.yaml
---
- name: Playbook to automatically run regular tasks
  hosts: proxy
  tasks:
    - name: Create a root crontab record to append date to /var/log/time.log every hour
      cron:
        name: cron-18
        minute: "0"
        hour: "*"
        job: "/bin/date >> /var/log/time.log"
        user: root

# ansible-playbook playbookLab18.yaml --syntax-check


Lab 19: Create a cron job that runs every minute on the localhost to perform backup tasks using Ansible.

Host: localhost
Task: Configure a cron job with the following specifications:
Cron Job Name: cronjob
Frequency: Every minute (0-59)
Command to Execute:
Create a directory at /var/backups on all managed hosts.
Archive the /etc directory into a compressed file named etc_backup.tar.gz using gzip format and place it in /var/backups.
The cron job should be defined in the root user's crontab to ensure that it has the necessary permissions to execute the backup tasks.

# vim playbookLab19.yaml


---
- name: Playbook to automatically run regular tasks
  hosts: localhost
  tasks:
    - name: Configure a cron job with the following specifications
      cron:
        name: cronjob
        minute: "*"
        user: root
        job: "mkdir -p /var/backups; tar -czvf /var/backups/etc_backup.tar.gz /etc" [gzip format]
        # job: "mkdir -p /var/backups; tar -cjvf /var/backups/etc_backup.tar.bz2 /etc" [bzip2 format]

# ansible-playbook playbookLab19.yaml --syntax-check



----------------------------------------------------------------------------------------------------------------------------------------------
Part X : TroubleMakers                                                                                                                        |
----------------------------------------------------------------------------------------------------------------------------------------------
Question (1): File Content
Create a playbook /home/automation/plays/motd.yml that runs on all inventory hosts and
does the following:
    ○ The playbook should replace any existing content of /etc/motd with text. Text
    depends on the host group.
    ○ On hosts in the proxy host group the line should be “Welcome to HAProxy server”.
    ○ On hosts in the webservers host group the line should be “Welcome to Apache server”.
    ○ On hosts in the database host group the line should be “Welcome to MySQL server”.

---
- name: Playbook to Create Custom Banner On specific Groups
  hosts: all
  become: true
  tasks:
    - lineinfile:
        path: /etc/motd
        line: Welcome to HAProxy server
      when: inventory_hostname in groups.proxy

    - lineinfile:
        path: /etc/motd
        line: Welcome to Apache server
      when: inventory_hostname in groups.webservers

    - lineinfile:
        path: /etc/motd
        line: Welcome to Apache MySQL server
      when: inventory_hostname in groups.database


Question (2): Configure SSH Server
Create a playbook /home/automation/plays/sshd.yml that runs on all inventory hosts and
configures SSHD daemon as follows:
    ○ banner is set to /etc/motd
    ○ X11Forwarding is disabled
    ○ MaxAuthTries is set to 3

# vim /home/automation/plays/sshd.yml

- name: playbook to configure custom ssh server
  hosts: all
  become: true
  tasks: 
    - lineinfile:
        path: /etc/ssh/sshd_config
        regexp: '^#Banner'
        line: Banner /etc/motd

    - lineinfile:
        path: /etc/ssh/sshd_config
        regexp: '^#X11Forwarding'
        line: X11Forwarding no

    - lineinfile:
        path: /etc/ssh/sshd_config
        regexp: '^#MaxAuthTries'
        line: MaxAuthTries 3


Question (3): Users Creation with Ansible Vault
Part 1:
Create Ansible vault file /home/automation/plays/secret.yml. Encryption/decryption
password is devops.
Add the following variables to the vault:
○ user_password with value of devops
○ database_password with value of devops
Store Ansible vault password in the file /home/automation/plays/vault_key

# ansible-vault create /home/automation/plays/secret.yml
New Vault password: devops
Confirm New Vault password: devops
user_password: "devops"
database_password: "devops"
# echo "devops" > /home/automation/plays/vault_key

Part 2:
You have been provided with the list of users below.
Use /home/automation/plays/vars/user_list.yml file to save this content.
---
users:
- username: alice
uid: 1201
- username: vincent
uid: 1202
- username: sandy
uid: 2201
- username: patrick
uid: 2202

# mkdir vars
# cd vars/
# vim user_list.yml
---
users:
- username: alice
  uid: "1201"
- username: vincent
  uid: "1202"
- username: sandy
  uid: "2201"
- username: patrick
  uid: "2202"

Part 3: 
Create a playbook /home/automation/plays/users.yml that uses the vault file
/home/automation/plays/secret.yml to achieve the following:
    ○ Users whose user ID starts with 1 should be created on servers in the webservers host
    group. User password should be used from the user_password variable.
    ○ Users whose user ID starts with 2 should be created on servers in the database host
    group. User password should be used from the database_password variable.
    ○ All users should be members of a supplementary group wheel.
    ○ Shell should be set to /bin/bash for all users.
    ○ Account passwords should use the SHA512 hash format.
    ○ Each user should have an SSH key uploaded (use the SSH key that you created
previously, see task #2).

After running the playbook, users should be able to SSH into their respective servers without passwords.

# vim /home/automation/plays/users.yml
---
- name: Create users with UID starting with 1 on webservers group
  hosts: webservers
  become: true
  vars_files:
    - /home/automation/plays/vars/user_list.yml
    - /home/automation/plays/secret.yml
  tasks:
    - user:
        name: "{{item.username}}"
        uid: "{{item.uid}}"
        groups: wheel
        ssh_key_file: /home/automation/.ssh/id_rsa
        shell: /bin/bash
        password: "{{ user_password | password_hash('sha512') }}"
      loop: "{{ users }}" 
      when: item.uid[0] == '1'
      # when: ( item.uid | string )[0] == '1' : in case of non-modification (toString(uid)) of user_list file

---
- name: Create users with UID starting with 2 on database group
  hosts: database
  become: true
  vars_files:
    - /home/automation/plays/vars/user_list.yml
    - /home/automation/plays/secret.yml
  tasks:
    - user:
        name: "{{item.username}}"
        uid: "{{item.uid}}"
        groups: wheel
        ssh_key_file: /home/automation/.ssh/id_rsa
        shell: /bin/bash
        password: "{{ database_password | password_hash('sha512') }}"
      loop: "{{ users }}"  
      when: item.uid[0] == '2'
      # when: ( item.uid | string )[0] == '2' : in case of non-modification (toString(uid)) of user_list file



Question (4): Create and Work with Roles
Create a role called sample-mysql and store it in /home/automation/plays/roles. The role
should satisfy the following requirements:
    ○ A primary partition number 1 of size 800MB on device /dev/sdb is created.
    ○ An LVM volume group called vg_database is created that uses the primary partition
    created above.
    ○ An LVM logical volume called lv_mysql is created of size 512MB in the volume group
    vg_database.
    ○ An XFS filesystem on the logical volume lv_mysql is created.
    ○ Logical volume lv_mysql is permanently mounted on /mnt/mysql_backups.
    ○ mysql-community-server package is installed.
    ○ Firewall is configured to allow all incoming traffic on MySQL port TCP 3306.
    ○ MySQL root user password should be set from the variable database_password (see
    task #5).
    ○ MySQL server should be started and enabled on boot.
    ○ MySQL server configuration file is generated from the my.cnf.j2 Jinja2 template with
    the following content:
            [mysqld]
            bind_address = {{ ansible_default_ipv4.address }}
            skip_name_resolve
            datadir=/var/lib/mysql
            socket=/var/lib/mysql/mysql.sock
            symbolic-links=0
            sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES
            [mysqld_safe]
            log-error=/var/log/mysqld.log
            pid-file=/var/run/mysqld/mysqld.pid
Create a playbook /home/automation/plays/mysql.yml that uses the role and runs on hosts in the database host group


[automation@control sample-mysql]$ tree
.
├── defaults
│ └── main.yml
├── files
├── handlers
│ └── main.yml
├── meta
│ └── main.yml
├── README.md
├── tasks
│ └── main.yml
├── templates
│ └── my.cnf.j2
├── tests
│ ├── inventory
│ └── test.yml
└── vars
    ├── main.yml
    ├── secret.yml
    └── vault_key

# vim tasks/main.yml
 parted:
    device: /dev/sdb
    number: 1
    state: present
    part_end: 801MiB

- lvg:
    vg: vg_database
    pvs: /dev/sdb1

- lvol:
    vg: vg_database
    lv: lv_mysql
    size: 512

- filesystem:
    fstype: xfs
    dev: /dev/vg_database/lv_mysql
    force: true

- file:
    path: /mnt/mysql_backups
    state: directory

- mount:
    path: /mnt/mysql_backups
    src: /dev/vg_database/lv_mysql
    fstype: xfs
    opts: defaults
    state: present

- dnf:
    name: mysql-community-server
    state: present
  ignore_errors: yes

- firewalld:
    port: 3306/tcp
    state: enabled
    permanent: true
    immediate: true
  ignore_errors: yes


- community.mysql.mysql_user:
    name: root
    password: "{{ database_password | password_hash('sha512') }}"
    state: present

- service:
    name: mysql
    state: started
    enabled: yes

- template:
    src: /home/automation/roles/sample-mysql/templates/my.cnf.j2
    dest: /etc/my.cnf

# vim vars/main.yml
secret: secret.yml


# ansible-vault view vars/secret.yml
Vault password: devops
user_password: "devops"
database_password: "devops"

# vim /templates/my.cnf.j2
[mysqld]
bind_address = {{ ansible_default_ipv4.address }}
skip_name_resolve
datadir=/var/lib/mysql
socket=/var/lib/mysql/mysql.sock
symbolic-links=0
sql_mode=NO_ENGINE_SUBSTITUTION,STRICT_TRANS_TABLES
[mysqld_safe]
log-error=/var/log/mysqld.log
pid-file=/var/run/mysqld/mysqld.pid

# vim /home/automation/plays/mysql.yml

---
- name: Playbook to run roles
  hosts: database
  become: yes
  roles:
    - sample-mysql

# ansible-playbook mysql.yml --inventory=/home/automation/plays/inventory



Question (5): Create a role called sample-apache and store it in /home/automation/plays/roles. The role
should satisfy the following requirements:
    ○ The httpd, mod_ssl and php packages are installed. Apache service is running and
    enabled on boot.
    ○ Firewall is configured to allow all incoming traffic on HTTP port TCP 80 and HTTPS port
    TCP 443.
    ○ Apache service should be restarted every time the file /var/www/html/index.html is
    modified.
    ○ A Jinja2 template file index.html.j2 is used to create the file

/var/www/html/index.html with the following content:
The address of the server is: IPV4ADDRESS
IPV4ADDRESS is the IP address of the managed node.

Create a playbook /home/automation/plays/apache.yml that uses the role and runs on hosts in the web servers host group.

# cd roles/
# ansible-galaxy role init sample-apache
# cd sample-apache/
# tree
.
├── defaults
│ └── main.yml
├── files
├── handlers
│ └── main.yml
├── meta
│ └── main.yml
├── README.md
├── tasks
│ └── main.yml
├── templates
├── tests
│ ├── inventory
│ └── test.yml
└── vars
    └── main.yml


# vim tasks/main.yml
- dnf:
    name:
      - httpd
      - mod_ssl
      - php
    state: present
  ignore_errors: yes  # Packages could be unavailable

- service:
    name: httpd
    state: started
    enabled: yes

- firewalld:
    service: "{{ item }}"
    state: enabled
    permanent: true
    immediate: true
  loop:
    - http
    - https

- template:
    src: templates/index.html.j2
    dest: /var/www/html/index.html
  notify: Restart Apache

# vim handlers/main.yml
- name: Restart Apache
  service:
    name: httpd
    state: restarted

# vim templates/index.html.j2
The address of the server is: {{ ansible_default_ipv4.address }}


# vim plays/apache.yml
---
- name: Playbook to run sample-apache Role
  hosts: webservers
  become: true
  roles:
    - sample-apache



Question (6): Use Ansible Galaxy to download and install geerlingguy.haproxy role in
/home/automation/plays/roles.
Create a playbook /home/automation/plays/haproxy.yml that runs on servers in the proxy
host group and does the following:
    ○ Use geerlingguy.haproxy role to load balance requests between hosts in the web
    servers host group.
    ○ Use roundrobin load balancing method.
    ○ HAProxy backend servers should be configured for HTTP only (port 80).
    ○ Firewall is configured to allow all incoming traffic on port TCP 80.
If your playbook works, then doing “curl http://ansible2.hl.local/” should return output from the web server. Running the command again should return output from the other web server.


# ansible-galaxy role install geerlingguy.haproxy -p /home/automation/plays/roles


Question (6): 
Create a playbook of any name, and that playbook should do as shown below:
The playbook runs over all the managed hosts and uses the time sync role ( Red Hat system role) This role should be able to change the time server to nl.pool.ntp.org .

Step 1: you must have RHEL system roles installed and settled all the paths.

# dnf install rhel-system-roles -y

Step 2: Copy the roles into your ansible directory.

# sudo cp -avr /usr/share/ansible/roles/rhel-system-roles.* /home/ansible/roles

Step 3: Copy /usr/share/doc/rhel-sytem-roles/timesync/example-timesync-playbook.yml to home directory.

# cp /usr/share/doc/rhel-system-roles/timesync/example-single-pool-playbook.yml /home/ansible/

Step 4: Edit the file has shome below:

# cd /home/ansible
# vim example-single-pool-playbook.yml

---
- name: single pool playbook
  hosts: all
  vars:
    timesync_ntp_servers:
      - hostname: nl.pool.ntp.org
        iburst: true
  roles:
    - rhel-system-roles.timesync


# mv example-single-pool-playbook.yml timesync.yml


Question (7): 
Create a file called requirements.yml in /home/ansible/roles to install two roles. The source for the first role is geerlingguy.haproxy and geerlingguy.php. Name the first haproxy-role and the second php-role. The roles should be installed in /home/ismat/ansible/roles.

# vim requirements.yml

- name: haproxy-role
  src: geerlingguy.haproxy

- name: php-role
  src: geerlingguy.php

# ansible-galaxy install -r requirements.yml -p /home/ansible/roles/

[!] Role will be available in the roles_path /home/ansible/roles/php-role & /home/ansible/roles/haproxy-role [!]


Question (8): 
(A):
Create a playbook that should perform the following tasks:
Create a primary partition on /dev/sdb disk with the size of 2GB.
Create a volume group named "RedHat".
Create an lv named "exam" with size 500Mb
Formate the new lv as xfs.
Create a new directory named /mydir on it.
Mount /mydir so it can sustain the reboot.
(B):
Use the remaining VG “RedHat” space and create a new LV named “ex294”.
Formate the new lv with ext4.
Create a folder named “/exam” and mount it so it can sustain the reboot.

---
- name: Playbook to answer Q8-A
  hosts: mgr
  become: true
  tasks: 
    - name: Creating partition /dev/sdd 
      community.general.parted:
        device: /dev/sdd
        number: 1
        state: present
        part_end: 2GiB

    - name: VG Creation
      lvg:
        vg: redhat
        pvs: /dev/sdd1

    - name: LV Creation 
      lvol:
        vg: redhat
        lv: exam
        size: 500M

    - name: Formatting the LV
      community.general.filesystem:
        fstype: xfs
        dev: /dev/redhat/exam

    - name: MP Creation
      ansible.builtin.file:
        path: /mydir
        state: directory

    - name: Persistant Mounting
      ansible.posix.mount:
        path: /mydir
        src: /dev/redhat/exam
        fstype: xfs
        state: mounted

- name: Playbook to answer Q8-B
  hosts: mgr
  become: true
  tasks:
    - name: New LV Creation
      community.general.lvol:
        vg: redhat
        lv: ex294
        size: 100%FREE

    - name: Formatting the LV
      community.general.filesystem:
        fstype: ext4
        dev: /dev/redhat/ex294

    - name: MP Creation
      ansible.builtin.file:
        path: /exam
        state: directory

    - name: Persistant Mounting
      ansible.posix.mount:
        path: /exam
        src: /dev/redhat/ex294
        fstype: ext4
        state: mounted

Question (9): 
(A):
Create an ansible vault password file called lock.yml with the password reallysafepw in the /home/ansible directory. In the lock.yml file define two variables. One is pw_dev and the password is ‘dev’ and the other is pw_mgr and the password is ‘mgr’. Create a regular file called secret.txt which contains the password for lock.yml.
(B):
Create the users in the file users_list.yml file provided. Do this in a playbook called users.yml located at /home/ansible. The passwords for these users should be set using the lock.yml file from (A). 
When running the playbook, the lock.yml file should be unlocked with secret.txt file from Question 7.

All users with the job of ‘developer’ should be created on the dev hosts, add them to the group devops, their password should be set using the pw_dev variable. Likewise create users with the job of ‘manager’ on the mgr host and add the users to the group ‘managers’, their password should be set using the pw_mgr variable.

(C):
15. Rekey the variable file vault.yml
Old password: P@sswOrd
New password: redhat


# vim users_list.yml
users:
 - username: bill
   job: developer
 - username: chris
   job: manager
 - username: dave
   job: test
 - username: ethan
   job: developer


(A'):
# ansible-vault create lock.yml
New Vault password:
Confirm New Vault password:
# vim /home/ansible/secret.txt
# cat /home/ansible/secret.txt
reallysafepw
# ansible-vault view lock.yml --vault-password-file=/home/ansible/secret.txt
pw_dev: dev
pw_mgr: mgr


(B'):
# vim users.yml
---
- name: Playbook to answer Q9-B Dev Part
  hosts: dev
  become: true
  vars_files:
    - lock.yml
    - users_list.yml
  tasks:
    - name: Devops Users Creation
      user:
        name: "{{ item.username }}"
        comment: "{{ item.job }}"
        group: devops
        password: "{{ pw_dev | password_hash('sha256') }}"
      when: item.job == 'developer'
      loop: "{{ users }}"


- name: Playbook to answer Q9-B Dev Part
  hosts: proxy
  become: true
  vars_files:
    - lock.yml
    - users_list.yml
  tasks:
    - name: Mgment Users Creation
      user:
        name: "{{ item.username }}"
        comment: "{{ item.job }}"
        group: managers
        password: "{{ pw_mgr | password_hash('sha256') }}"
      when: item.job == 'manager'
      loop: "{{ users }}"

# ansible-playbook users.yml --vault-password-file secret.txt

(C'):
# ansible-vault rekey vault.yml 
Vault password: P@sswOrd
New Vault password: redhat
Confirm New Vault password: redhat
Rekey successful

Question (10): 
Create a file in /home/ansible/ called report.yml. Using this playbook, get a file called report.txt. Download the file from http://classroom.example.com to all remote hosts at /root/report.txt. Then edit the lines in the file to provide the real information of the hosts. If a disk does not exist then write NONE. 

The file content of report.txt is
HOST= inventory hostname
MEMORY=total memory in mb
BIOS=bios version
SDA_DISK_SIZE=disk size
SDB_DISK_SIZE=disk size

# vim report.yml

---
- name: Playbook to extract hosts information
  hosts: all
  become: true
  tasks:
    - name: Download report.txt
      ansible.builtin.get_url:
        url: http://classroom.example.com
        dest: /root/report.txt

    - name: Editing line-1
      lineinfile:
        path: /root/report.txt
        regexp: '^HOST='
        line: HOST= "{{ ansible_hostname }}"

    - name: Editing line-2
      lineinfile:
        path: /root/report.txt
        regexp: '^MEMORY='
        line: MEMORY= "{{ ansible_memtotal_mb }}"

    - name: Editing line-3
      lineinfile:
        path: /root/report.txt
        regexp: '^BIOS='
        line: BIOS= "{{ ansible_bios_version }}"

    - name: Editing line-4
      lineinfile:
        path: /root/report.txt
        regexp: '^SDA_DISK_SIZE'
        line: SDA_DISK_SIZE= "{{ ansible_devices.sda.size }}"

    - name: Editing line-5
      lineinfile:
        path: /root/report.txt
        regexp: '^SDB_DISK_SIZE'
        line: SDB_DISK_SIZE= "{{ ansible_devices.sdb.size }}"


Question (11): 
Create a playbook that meets following requirements:
    - Is placed at /home/automation/plays/system_control.yml
    - Runs against all hosts
    - If a server has more than 1024MB of RAM, then use sysctl module to set vm.swappiness to 10
    - If a server has less or equal to 1024MB of RAM exist with error message Server has less than required 1024MB of RAM
    - Configuration change should survive server reboots

# vim system_control.yml
---
- name: Playbook to control system
  hosts: all
  become: true
  vars:
    - threshold: 1024
  tasks:
    - name: sysctl set to value-10 if RAM > threshold
      ansible.posix.sysctl:
        name: vm.swappiness
        value: "10"
        sysctl_set: true
        reload: true
      when: ansible_memtotal_mb > threshold

    - name: error msg if else
      ansible.builtin.fail:
        msg: Server has less than required 1024MB of RAM
      when: ansible_memtotal_mb <= threshold

Question (12): 
Download the jinja template from http://dl.example.com/hosts.j2 in /home/ansible/ and Edit this file so it looks like the one below. The order of the nodes doesn’t matter. Then create a playbook in /home/ansible called hosts.yml and install the template on dev node at /root/myhosts.

127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4
::1 localhost localhost.localdomain localhost6 localhost6.localdomain6
10.0.2.1 node1.example.com node1
10.0.2.2 node2.example.com node2
10.0.2.3 node3.example.com node3
10.0.2.4 node4.example.com node4
10.0.2.5 node5.example.com node5


# vim hosts.j2 
127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4
::1 localhost localhost.localdomain localhost6 localhost6.localdomain6
{% for host in groups.all %}
{{ hostvars[host].ansible_default_ipv4.address }} {{ hostvars[host].ansible_fqdn }} {{ hostvars[host].ansible_hostname }}
{% endfor %}

# vim hosts.yml 
---
- name: Playbook to apply jinja template
  hosts: all 
  become: true
  tasks:
    - name: Application
      template:
        src: /home/automation/plays/hosts.j2
        dest: /root/myhosts
      when: inventory_hostname in groups.dev 


Question (13): 

(A)
Create Ansible vault file /home/automation/plays/secret.yml. Encryption/decryption password is devops.

Add the following variables to the vault:
user_password with value of devops 
database_password with value of devops
Store Ansible vault password in the file /home/automation/plays/vault_key.


# ansible-vault create secret.yml
# echo "devops" > vault_key 
# ansible-vault view secret.yml --vault-id=vault_key
Result:
user_password: "devops"
database_password: "devops"


(B)
You have been given defintions of variables for the task

---
users:
- username: adam
  uid: 2000
- username: greg
  uid: 2001
- username: robby
  uid: 3001
- username: xenia
  uid: 3002

Store a file with the above content at /home/automation/plays/vars/users.yml
Create a playbook that meets following requirements:
Create a playbook /home/automation/plays/users.yml that uses the vault file /home/automation/plays/secret.yml to achieve the following:
Creates users whose uid starts with 2 on webservers host group. Password should - be taken from the variable stored at /home/automation/plays/secret.yml (created in previous exercise)

Creates users whose uid starts with 3 on database host group. Password should - be taken from the variable stored at /home/automation/plays/secret.yml (created in previous exercise)
Users should be part of supplementary group wheel
Users' shell should be set to /bin/bash
Password should use SHA512 hash format
Each user should have an SSH key uploaded (use the SSH key that you created previously, see task #2)
After running the playbook users should be able to SSH into their servers - without providing password to prompt

# vim /home/automation/plays/vars/users.yml
---
users:
- username: adam
  uid: "2000" # toString()
- username: greg
  uid: "2001" # toString()
- username: robby
  uid: "3001" # toString()
- username: xenia
  uid: "3002" # toString()

# vim /home/automation/plays/vars/users.yml
---
- name: Playbook
  hosts: webservers
  become: true
  vars_files:
    - /home/automation/plays/secret.yml
    - /home/automation/plays/vars/users.yml
  tasks:
    - name:
      user:
        name: "{{ item.username }}"
        password: "{{ user_password | password_hash('sha512') }}"
      loop: "{{ users }}"
      when: item.uid[0] == '2'

- name: Playbook
  hosts: database
  become: true
  vars_files:
    - /home/automation/plays/secret.yml
    - /home/automation/plays/vars/users.yml
  tasks:
    - name:
      user:
        name: "{{ item.username }}"
        password: "{{ database_password | password_hash('sha512') }}"
        groups: wheel
        shell: /bin/bash
        ssh_key_file: .ssh/id_rsa
        generate_ssh_key: yes
      loop: "{{ users }}"
      when: item.uid[0] == '3'

    - name:
      ansible.posix.authorized_key:
        user: "{{ item.username }}"
        state: present
        key: "{{ lookup('file', '/home/automation/.ssh/id_rsa.pub') }}"
      loop: "{{ users }}"
      when: item.uid[0] == '3'

Question (14): 
Create a playbook webcontent.yml and it should run on dev group.
Create a directory /devweb and it should be owned by devops group.
Directory /devweb should have context type as "httpd"
Assign the permission for user=rwx,group=rwx,others=rx and group special permission should be applied to /devweb.
Create an index.html file under /devweb directory and the file should have the content "Developement".
Link the /devweb directory to /var/www/html/devweb.

---
- name: Playbook
  hosts: dev
  become: true
  tasks:
    - name: Dev Web Directory Creation
      file:
        path: /devweb
        state: directory
        group: devops   # Ensure that devops group exist on dev hosts
        mode: '2775' # g+s=2
        setype: httpd_sys_content_t  # To memorize

    - name: index.html Creation
      file:
        path: /devweb/index.html
        state: touch  # Warning

    - name: index.html Content
      copy:
        content: 'Development'
        dest: /devweb/index.html

    - name: Linking /var/www/html with /devweb
      file:
        src: /devweb
        dest: /var/www/html/devweb
        state: link


Question (15): 
Collect hardware report using playbook in all nodes.

  i)  Download hwreport.txt from the url http://content.example.com/Rhce/hwreport.txt and save it under /root.

  /root/hwreport.txt should have the content with node informations as key=value.
  #hwreport
  HOSTNAME=
  MEMORY=
  BIOS=
  CPU=
  DISK_SIZE_VDA=
  DISK_SIZE_VDB=

  ii)  If there is no information it has to show "NONE".
  iii) playbook name should be hwreport.yml.
 
__________________________________________________________________________________________________________________________

Note: Copy the url "http://content.example.com/Rhce/hwreport.txt" and paste that on new tab and verify the content in it.
__________________________________________________________________________________________________________________________

# vim hwreport.yml

---
- become: true
  hosts: all
  tasks:
    - get_url:
        url: http://example.com/path/hwreport.txt
        dest: /root/hwreport.txt
      ignore_errors: yes
    - copy:
        content: |
          HOSTNAME= {{ ansible_hostname | default('NONE') }}
          MEMORY= {{ ansible_memtotal_mb | default('NONE')  }}
          BIOS= {{ ansible_bios_version | default('NONE')  }}
          CPU= {{ CPU | default('NONE') }}
          DISK_SIZE_SDA= {{ DISK_SIZE_SDA | default('NONE') }}
          DISK_SIZE_SDB= {{ DISK_SIZE_SDB | default('NONE') }}
        dest: /root/hwreport.txt



Question (16): 
Create a playbook /home/automation/plays/target.yml that runs on hosts in the webservers
host group and does the following:
-> Sets the default boot target to multi-user

# vim target.yml
---
- name: Playbook to configure default boot target to multi-user
  hosts: webservers
  become: yes
  tasks:
    - name: on boot unit
      ansible.builtin.systemd:
        state: started
        name: multi-user.target
        enabled: yes



Question (17): 
Create a playbook /home/automation/plays/server_list.yml that does the following:
○ Playbook uses a Jinja2 template server_list.j2 to create a file
/etc/server_list.txt on hosts in the database host group.
○ The file /etc/server_list.txt is owned by the automation user.
○ File permissions are set to 0600.
○ SELinux file label should be set to net_conf_t.
○ The content of the file is a list of FQDNs of all inventory hosts.
After running the playbook, the content of the file /etc/server_list.txt should be the following:
ansible2.hl.local
ansible3.hl.local
ansible4.hl.local
ansible5.hl.local

Note: if the FQDN of any inventory host changes, re-running the playbook should update the file with the new values.


# vim server_list.yml 
---
- name: Playbook to create a custom /etc/hosts 
  hosts: database
  become: true
  tasks: 
    - name: Application
      template:
        src: server_list.j2
        dest: /etc/server_list.txt
        owner: automation
        mode: '0600'

    - name: Selinux 
      file: 
        path: /etc/server_list.txt
        setype: net_conf_t

# vim server_list.j2 
{% for host in groups['all'] %}
{{ hostvars[host].ansible_fqdn }}
{% endfor%}


Question (18): 
Create a playbook /home/rhce/exam/selinux.yml that runs on hosts in the dev host group and
does the following:
○ Enables httpd_can_network_connect SELinux boolean.
○ The change must survive system reboot.


---
- become: true
  hosts: proxy
  tasks:
    - ansible.posix.seboolean:
        name: httpd_can_network_connect
        state: true
        persistent: true


Question (19): 
Create a logical volume named data of 1500M size from the volume group research
and if 1500M size is not created, then atleast it should create 800M size.
i) Verify if vg not exist, then it should debug msg "vg not found" .
ii) 1500M lv size is not created, then it should debug msg "Insufficient size of vg" .
iii) If Logical volume is created, then assign file system as "ext3" .
iv) Do not perform any mounting for this LV.
iv) The playbook name lvm.yml and run the playbook in all nodes

# vim lvm.yml

- become: true
  hosts: all
  tasks:
    - block:
        - debug:
            msg: "VG NOT FOUND"
      when: ansible_lvm.vgs.research is not defined
    - block:
        - name: Create a logical volume
          community.general.lvol:
            vg: research
            lv: data
            size: 1500

      rescue:
        - debug:
            msg: "INSUFFICIENT SIZE OF VG"
        - name: Create a logical volume
          community.general.lvol:
            vg: research
            lv: data
            size: 800
      always:
        - name: Create a ext2 filesystem on /dev/sdb1
          community.general.filesystem:
            fstype: ext3
            dev: /dev/research/data
      when: ansible_lvm.vgs.research is defined


Question (20):
Task-1:
Create an Ansible playbook named playbookLab7-init.yaml that performs the following tasks on hosts belonging to the "lvm_group" group.
For the host ansible1:
Create a new partition of size 2GiB on /dev/sdb.

For the host ansible3:
Create a new partition of size 1GiB on /dev/sdb.

After creating the partitions on both hosts, create a Volume Group (VG) named ansible_vg using the partition /dev/sdb1 as the physical volume.


Task-2:
Create an Ansible playbook named playbookLab7.yaml that performs the following tasks on hosts belonging to the lvm_group group.
Verify the following conditions:

Fail the task if the host does not have the device /dev/sdb.
Fail the task if the Volume Group ansible_vg does not exist.
Fail the task if there is not enough free space (at least 1.5GiB) in the Volume Group.
If the above conditions are met, proceed to create a Logical Volume (LV) named ansible_lv with a size of 1500MiB. If this fails due to insufficient space, attempt to create the LV with a size of 800MiB instead.

Regardless of the outcome of the previous tasks, ensure that:

A directory is created at /lv/.
The Logical Volume is formatted with the ext4 filesystem.
The Logical Volume is mounted at /lv, ensuring that it uses the defaults options.
Ensure that all tasks are conditionally executed only on hosts in the lvm_group group.  

# vim playbookLab7-T1.yaml

--- 
- name: Playbook for partitionning ansible1 and ansible3 to inint for the playbookLab7.yaml
  hosts: lvm_group
  tasks:

# ansible1 has vg “ansible_vg” uses /dev/sdb1 with size 2GB
# ansible3 has vg “ansible_vg” uses /dev/sdb1 with size 1GB

    - name: creating partition on ansible1
      parted:
        device: /dev/sdb
        number: 1
        state: present
        part_end: 2GiB
      when: ansible_hostname == "ansible1"

    - name: creating partition on ansible3
      parted:
        device: /dev/sdb
        number: 1
        state: present
        part_end: 1GiB
      when: ansible_hostname == "ansible3"
    
    - name: creating a vg for both hosts
      lvg:
        pvs: /dev/sdb1
        vg: ansible_vg

# ansible-playbook playbookLab7-T1.yaml --syntax-check 

# vim playbookLab7-T2.yaml

---
- name: Playbook to create LVM partitions on specific hosts
  hosts: lvm_group
  tasks:
      - fail: 
          msg: "Host does NOT have /dev/sdb"
        when: ansible_devices.sdb is not defined 

      - fail:
          msg: "ansible_vg does NOT exist"
        when: ansible_lvm.vgs.ansible_vg is not defined

    - block:

      - fail:
          msg: "Not enough space"
        when: (ansible_lvm.vgs.ansible_vg.free_g | float) < 1.5

      - lvol:
          vg: ansible_vg
          lv: ansible_lv
          size: 1500
      rescue:
      - lvol:
          vg: ansible_vg
          lv: ansible_lv
          size: 800

      always:
      - file:
          path: /lv/
          state: directory
          mode: '0777'

      - filesystem:
          fstype: ext4
          dev: /dev/ansible_vg/ansible_lv

      - mount:
          path: /lv/
          src: /dev/ansible_vg/ansible_lv
          fstype: ext4
          opts: defaults
          state: present


# ansible-playbook playbookLab7-T2.yaml --syntax-check 

Question (21):
Create a role called sample-apache in /home/ismat/ansible/roles that enables and starts httpd, enables and starts the firewall; and allows the webserver service. Create a template called index.html.j2 which creates and serves a message from /var/www/html/index.html. Whenever the content of the file changes, restart the webserver service.
Welcome to [FQDN] on [IP]
Replace the FQDN with the fully qualified domain name and IP with the ip address of the node using ansible facts. Lastly, create a playbook in /home/ismat/ansible named apache.yml and use the role to serve the index file on webserver hosts.


# ansible-galaxy init sample-apache

# vim sample-apache/tasks/main.yml
---
# tasks file for sample-apache
- name: Enable httpd
  service:
   name: httpd
   state: started
   enabled: yes
- name: Enable firewalld
  service: 
   name: firewalld
   state: started
   enabled: yes
- name: Allow webserver service
  firewalld:
   service: http
   state: enabled
   permanent: yes
   immediate: yes
- name: Create index file from index.html.j2
  template:
   src: index.html.j2
   dest: /var/www/html/index.html
  notify:
   - restart_webservers

# vim sample-apache/templates/index.html.j2

Welcome to {{ ansible_fqdn }} on {{ ansible_default_ipv4.address }}


# vim sample-apache/handlers/main.yml

- name: restart_webservers
  service:
   name: httpd
   state: restarted

# vim /home/automation/ansible and write in apache.yml

---
- name: Install apachec from apache-role 
  hosts: webservers
  roles: 
   - name: sample-apache
...


Question (22): 
Task-A
Create a file called requirements.yml in /home/automation/ansible/roles to install two roles. The source for the first role is geerlingguy.haproxy and the second geerlingguy.php. Name the first haproxy-role and the second php-role. The roles should be installed in /home/ismat/ansible/roles.

Task-B
Use the roles from Task A in a file called role.yml in /home/automation/ansible/. The haproxy-role should be used on the proxy host. And when you curl http://node3.example.com. it should display “Welcome to node4.example.com” and when you curl again “Welcome to node5.example.com” The php-role should be used on the prod host.

# vim  /home/automation/ansible/roles
- name: haproxy-role
  src: geerlingguy.haproxy

- name: php-role
  src: geerlingguy.php 

# ansible-galaxy install -r requirements.yml -p /home/automation/ansible/roles

# vim /home/automation/ansible/roles.yml

---
- name: install haproxy and php roles
  hosts: proxy
  become: true
  vars:
   haproxy_backend_servers:
    - name: web1
      address: node4.example.com:80
    - name: web2
      address: node5.example.com:80 
  roles:
   - haproxy


- name: install haproxy and php roles
  hosts: prod
  become: true
  roles:
   - php-role



Question (23):
Create and run an Ansible ad-hoc command. As a system administrator, you will need to
install software on the managed nodes:
a) Create a shell script called yum-repo.sh that runs Ansible ad-hoc commands to create the
yum repositories on each of the managed nodes as per the following details:
b) NOTE: you need to create 2 repos (BaseOS & AppStream) in the managed nodes.
BaseOS:
name: BaseOS
baseurl: file:///mnt/BaseOS/
description: Base OS Repo
gpgcheck: yes
enabled: yes
key: file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release
AppStream:
name: AppStream
baseurl: file:///mnt/AppStream/
description: AppStream Repo
gpgcheck: yes
enabled: yes
key: file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release

# vim yum-repo.sh

ansible all -m yum_repository -a "name=BaseOS baseurl='file:///mnt/BaseOS/' description='Base OS Repo' gpgcheck=yes gpgkey='file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release' enabled=yes" -i inventory

ansible all -m yum_repository -a "name=AppStream baseurl=file:///mnt/AppStream/ description='AppStream Repo' gpgcheck=yes gpgkey='file:///etc/pki/rpm-gpg/RPM-GPG-KEY-redhat-release' enabled=yes" -i inventory


╔═╗┌─┐┌─┐┌┬┐  ┬  ┬ ┬┌─┐┬┌─
║ ╦│ ││ │ ││  │  │ ││  ├┴┐
╚═╝└─┘└─┘─┴┘  ┴─┘└─┘└─┘┴ ┴
                        @Yassine_Sahli
                                             
                               